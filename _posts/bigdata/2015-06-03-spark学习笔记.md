---
title: spark 学习笔记
date: 2015-06-03 18:12:28
tags: bigdata
- a
- b
---

* toc 
{:toc}

### 配置

[spark 1.3的官网](http://spark.apache.org/docs/latest/index.html){:target="_blank"}上介绍了spark集群的几种方式：
Amazon EC2  
Standalone Deploy mode  
Apache Mesos  
Hadoop YARN  
因为平时都在使用hadoop，在测试时为了更好的了解安装部署的情况，就使用yarn的方式。

spark有两种运行的方式，一种是提交任务到master，spark管理任务的执行(使用命令spark-submit)；还有一种就是在客户端交互的方式运行，使用命令(spark-sell或pyspark，其中pyspark是python专用的)

关于spark的配置，可以在启动时通过命令的方式传递，如

    ./bin/spark-submit --name "My app" --master local[4] --conf spark.shuffle.spill=false 
   --conf "spark.executor.extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps" myApp.jar

如果没有这种动态的配置，spark会使用 conf/spark-defaults.conf配置文件中的内容  
配置情况如：

    spark.master                     spark://172.16.101.13:7077
    spark.eventLog.enabled           true
    spark.eventLog.dir               hdfs://hadoop-master:9000/tmp/spark_events
    spark.serializer                 org.apache.spark.serializer.KryoSerializer
    spark.driver.memory              1g
    spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value

网上有关于该文件的说明，这里只是说明 spark.eventLog.dir配置，后面的域名和hadoop的core-site.xml中的`fs.default.name`的value一致。  
有了该配置文件后，直接启动 pyspark 时，在最后会有输出：

    SparkContext available as sc, HiveContext available as sqlCtx

这种情况下，我们使用使用变量`sc`就可以了。本人在测试时，如果再次试图初始化(sc=SparkContext())会报错 `ValueError: Cannot run multiple SparkContexts at once`

###sparksql操作json文件

spark使用DATAFrame，其来源可以是存在的rdd、hive表、数据源。最简单而轻言，json格式的文件就可以直接使用。总的来看，支持的比较好. 
参考[Spark SQL and DataFrame Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html){:target="_blank"}和[http://www.cstor.cn/textdetail_8471.html](http://www.cstor.cn/textdetail_8471.html){:target="_blank"}。  
假设我有push上传的json格式日志，每一行都如下：

    {"event_id":1,"body":{"brand":"samsung","imsi":"454197000461112","dvid":"45yuifghjmd","cc":"HK","osv":"5.0","dtn":"SM-N9005","sr":"1920,1080","mac":"48:5A:3F:23:09:E1","platform":"android","net":"wifi"}}

该日志文件已经存放在hdfs的/mjyun/txt/push_json.log。

    pyspark
    >>> from pyspark.sql import SQLContext
    >>> sqlContext = SQLContext(sc)
    >>> df = sqlContext.jsonFile("/mjyun/txt/push_json.log")
    >>> df.select("event_id").show()
    >>> df.select("body.brand").show()  #json嵌套
    
求活跃设备数，即对dvid的去重数。

    pyspark
    >>> from pyspark.sql import SQLContext
    >>> sqlContext = SQLContext(sc)
    >>> event = sqlContext.jsonFile("/mjyun/txt/push_json.log")
    >>> event.registerTempTable("event")
    >>> cnt = sqlContext.sql("select count(distinct body.dvid) from event")
    >>> for value in cnt.collect():
    ...     print value
    ...
        Row(c0=964)

如果包含多个文件呢？在引入文件的时候，使用逗号隔开，直接包含多个文件即可，如下：

    >>>  df = sqlContext.jsonFile("/mjyun/txt/push_json.log,/mjyun/txt/push2.json")
    

### 结果存放

假设要计算新增设备数，则需要将以前出现过的设备都做存储，然后将本次的设备和之前出现的设备做比较。这里的第一步就需要先保存结果！
如上面的文件加载到了内存后

    >>> df.select("body.dvid","body.net").save("/mjyun/mid/devNet.parquet") 
    或者(支持 json, parquet, jdbc)
    >>> df.select("body.dvid","body.net").save("/mjyun/mid/devNet.json","json")
    重新加载
    >>> newdf = sqlContext.load("/mjyun/mid/devNet.json","json")
    
无锁非原子性，所以可能就会有错误。而在存储的时候，为了有更好的性能，采用分区方式存放(类似于hive分区)。  
例如在我们的应用的可以是 /mjyun/txt/appid=1222/ym=201506/day=02/  
支持数据的合并  

    
###提交任务

    $ cat new_dev.py 
    from pyspark import SQLContext, SparkContext, SparkConf
    conf = SparkConf().setAppName("testNewDevCnt").setMaster("spark://172.16.101.13:7077")
    sc = SparkContext(conf=conf)
    sqlContext = SQLContext(sc)

    df = sqlContext.jsonFile("/mjyun/txt/push2.json")
    df.select("body.ckid").save("appDevVer.parquet")
    sc.stop()
    
    $ spark-submit ./new_dev.py 
